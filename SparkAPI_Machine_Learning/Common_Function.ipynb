{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ecd561-e52b-4f76-87ca-87baacc93b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Version | Date       | Developer | Remark             |\n",
    "|---------|------------|-----------|--------------------|\n",
    "| 1.0     | Feb-1-2025 | Johnson | Initial version:developed pipeline & function for data engineering    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4ddf1d-2a9c-44e5-b139-8a3039f6eff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install synapseml\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e809fa-c387-4877-9827-0dc4c14cc387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np; np.__version__ = '1.24.0'\n",
    "import shap\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection  import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3639b1-26a4-47bc-8c82-a84b7c51bbf1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pipeline_Data_ Engineering"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Step 1: StringIndexer for categorical columns\n",
    "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_index\")\n",
    "smoker_indexer = StringIndexer(inputCol=\"smoker\", outputCol=\"smoker_index\")\n",
    "region_indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n",
    "\n",
    "# Step 2: OneHotEncoder for region\n",
    "region_encoder = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
    "\n",
    "# Step 3: VectorAssembler to combine \"bmi\" for MinMaxScaler\n",
    "bmi_assembler = VectorAssembler(inputCols=[\"bmi\"], outputCol=\"bmi_assembled\")\n",
    "age_assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"age_assembled\")\n",
    "\n",
    "# Step 4: MinMaxScaler for normalization\n",
    "bmi_scaler = MinMaxScaler(inputCol=\"bmi_assembled\", outputCol=\"bmi_normalized\")\n",
    "age_scaler = MinMaxScaler(inputCol=\"age_assembled\", outputCol=\"age_normalized\")\n",
    "\n",
    "# Step 5: Create Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    sex_indexer, smoker_indexer, region_indexer, region_encoder,  # Categorical encoding\n",
    "    bmi_assembler, bmi_scaler,  # bmi normalization\n",
    "    age_assembler, age_scaler   # age normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ecea318-cf67-4d29-bf76-29e48d689943",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VectorAssembler"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def vectorize_features(df_transformed):\n",
    "    # Step 1: Select the features to vectorize\n",
    "    input_columns = [\"sex_index\", \"smoker_index\", \"region_encoded\", \"bmi_normalized\", \"age_normalized\", \"children\"]\n",
    "\n",
    "    # Step 2: Create a new VectorAssembler to combine features\n",
    "    assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "\n",
    "    # Step 3: Rename expenses to label\n",
    "    df_transformed = df_transformed.withColumnRenamed(\"expenses\", \"label\")\n",
    "\n",
    "    # Step 4: Vectorize the features using VectorAssembler\n",
    "    df_vector = assembler.transform(df_transformed)\n",
    "    df_final = df_vector.select(\"label\", \"features\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4573d065-f760-43c0-bf9a-4493ab38783e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "plot_learning_curve"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(rf, para, start, end, step,cv=3):\n",
    "    results = []\n",
    "    for i in range(start, end, step):\n",
    "        rf.set_params(**{para: i})\n",
    "        scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='r2')\n",
    "        results.append(scores.mean())\n",
    "    plt.plot(range(start, end, step), results, color=\"red\", label=\"r2\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{para}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce8e5b7-db6b-4fb9-bc13-0b966000ca24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shap_Global"
    }
   },
   "outputs": [],
   "source": [
    "def show_shap_summary(rf, X_train, X_test, feature_names):\n",
    "    explainer = shap.Explainer(rf, X_train)\n",
    "    shap_values = explainer(X_test, check_additivity=False)\n",
    "    shap.summary_plot(shap_values, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57e9c6d-d5b7-4a30-b570-89617f8611bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shap_sample"
    }
   },
   "outputs": [],
   "source": [
    "def show_shap_sample(X_train, X_test,feature_names, sample_idx):\n",
    "    explainer = shap.Explainer(rf, X_train)\n",
    "    shap_values = explainer(X_test, check_additivity=False)\n",
    "    shap_values_for_sample = shap_values[sample_idx]\n",
    "    return shap.force_plot(explainer.expected_value, shap_values_for_sample.values, X_test[sample_idx], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5beee2e4-10a4-41c6-95ce-cbd170193b52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "application_predict_expenses"
    }
   },
   "outputs": [],
   "source": [
    "def application_predict_expenses(path_saved, age, sex, bmi, children, smoker, region, expenses):\n",
    "    loaded_model = PipelineModel.load(path_saved)\n",
    "    \n",
    "    new_data = spark.createDataFrame([(age, sex, bmi, children, smoker, region, expenses)], [\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\", \"expenses\"])\n",
    "    \n",
    "    predictions = loaded_model.transform(new_data)\n",
    "    \n",
    "    expected_expenses = predictions.withColumn(\"Expected_Expenses\", round(\"prediction\", 2))\n",
    "    expected_expenses = expected_expenses.withColumn(\"Actual_Expenses\", round(\"expenses\", 2))\n",
    "\n",
    "    expected_expenses = expected_expenses.withColumn(\"Claim_Status\", when(col(\"Expected_Expenses\") < col(\"Actual_Expenses\"), \"Abnormal Claim\").otherwise(\"Normal Claim\"))\n",
    "    \n",
    "    display(expected_expenses.select(\"Claim_Status\", \"Expected_Expenses\", \"Actual_Expenses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c99bf6d-ddb9-47ab-8655-b44972b12f87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "For_Local_Notebook"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "\n",
    "# def display(df):\n",
    "#     \"\"\"\n",
    "#     Mimics Databricks' display() function by converting a Spark DataFrame to Pandas \n",
    "#     and displaying it nicely in Jupyter notebooks.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pyspark.sql.DataFrame): The Spark DataFrame to display.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         from IPython.display import display as ipy_display\n",
    "#         ipy_display(df.toPandas())  # Convert to Pandas and display\n",
    "#     except:\n",
    "#         print(df.show())  # Fallback to show() if Pandas conversion fails"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_Function",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}