{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ecd561-e52b-4f76-87ca-87baacc93b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Version | Date       | Developer | Remark             |\n",
    "|---------|------------|-----------|--------------------|\n",
    "| 1.0     | Feb-1-2025 | Johnson | Initial version:developed pipeline & function for data engineering    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4ddf1d-2a9c-44e5-b139-8a3039f6eff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\r\n  Using cached shap-0.47.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (980 kB)\r\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from shap) (1.4.2)\r\nCollecting tqdm>=4.27.0\r\n  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\nCollecting slicer==0.0.8\r\n  Using cached slicer-0.0.8-py3-none-any.whl (15 kB)\r\nCollecting cloudpickle\r\n  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\r\nRequirement already satisfied: packaging>20.9 in /databricks/python3/lib/python3.9/site-packages (from shap) (21.3)\r\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from shap) (4.1.1)\r\nCollecting numba>=0.54\r\n  Using cached numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from shap) (1.7.3)\r\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (from shap) (1.0.2)\r\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from shap) (1.21.5)\r\nCollecting numpy\r\n  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\r\nCollecting llvmlite<0.44,>=0.43.0dev0\r\n  Using cached llvmlite-0.43.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>20.9->shap) (3.0.4)\r\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->shap) (2.8.2)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->shap) (2021.3)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->shap) (1.1.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn->shap) (2.2.0)\r\nCollecting numpy\r\n  Using cached numpy-1.22.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\nInstalling collected packages: numpy, llvmlite, tqdm, slicer, numba, cloudpickle, shap\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.21.5\r\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'numpy'. No files were found to uninstall.\r\nSuccessfully installed cloudpickle-3.1.1 llvmlite-0.43.0 numba-0.60.0 numpy-1.22.4 shap-0.47.0 slicer-0.0.8 tqdm-4.67.1\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting pycaret\r\n  Using cached pycaret-3.3.2-py3-none-any.whl (486 kB)\r\nCollecting pyod>=1.1.3\r\n  Using cached pyod-2.0.3-py3-none-any.whl\r\nRequirement already satisfied: numba>=0.55.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/lib/python3.9/site-packages (from pycaret) (0.60.0)\r\nCollecting psutil>=5.9.0\r\n  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\r\nCollecting imbalanced-learn>=0.12.0\r\n  Using cached imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\r\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/lib/python3.9/site-packages (from pycaret) (3.1.1)\r\nCollecting lightgbm>=3.0.0\r\n  Using cached lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\r\nRequirement already satisfied: numpy<1.27,>=1.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/lib/python3.9/site-packages (from pycaret) (1.22.4)\r\nRequirement already satisfied: ipywidgets>=7.6.5 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (7.7.2)\r\nCollecting pmdarima>=2.0.4\r\n  Using cached pmdarima-2.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\r\nCollecting deprecation>=2.1.0\r\n  Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\r\nRequirement already satisfied: matplotlib<3.8.0 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (3.5.1)\r\nCollecting scikit-learn>1.4.0\r\n  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\r\nCollecting schemdraw==0.15\r\n  Using cached schemdraw-0.15-py3-none-any.whl (106 kB)\r\nCollecting category-encoders>=2.4.0\r\n  Using cached category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\r\nCollecting wurlitzer\r\n  Using cached wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\r\nRequirement already satisfied: nbformat>=4.2.0 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (5.3.0)\r\nCollecting kaleido>=0.2.1\r\n  Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\nCollecting xxhash\r\n  Using cached xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\r\nRequirement already satisfied: requests>=2.27.1 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (2.27.1)\r\nCollecting joblib<1.4,>=1.2.0\r\n  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\r\nCollecting plotly>=5.14.0\r\n  Using cached plotly-6.0.0-py3-none-any.whl (14.8 MB)\r\nRequirement already satisfied: scipy<=1.11.4,>=1.6.1 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (1.7.3)\r\nCollecting scikit-plot>=0.3.7\r\n  Using cached scikit_plot-0.3.7-py3-none-any.whl (33 kB)\r\nRequirement already satisfied: ipython>=5.5.0 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (8.5.0)\r\nCollecting jinja2>=3\r\n  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\r\nCollecting tbats>=1.1.3\r\n  Using cached tbats-1.1.3-py3-none-any.whl (44 kB)\r\nRequirement already satisfied: markupsafe>=2.0.1 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (2.0.1)\r\nCollecting sktime==0.26.0\r\n  Using cached sktime-0.26.0-py3-none-any.whl (21.8 MB)\r\nCollecting yellowbrick>=1.4\r\n  Using cached yellowbrick-1.5-py3-none-any.whl (282 kB)\r\nRequirement already satisfied: statsmodels>=0.12.1 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (0.13.2)\r\nCollecting importlib-metadata>=4.12.0\r\n  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\r\nCollecting plotly-resampler>=0.8.3.1\r\n  Using cached plotly_resampler-0.10.0-py3-none-any.whl (80 kB)\r\nRequirement already satisfied: tqdm>=4.62.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/lib/python3.9/site-packages (from pycaret) (4.67.1)\r\nRequirement already satisfied: pandas<2.2.0 in /databricks/python3/lib/python3.9/site-packages (from pycaret) (1.4.2)\r\nCollecting scikit-learn>1.4.0\r\n  Using cached scikit_learn-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\r\nCollecting scikit-base<0.8.0\r\n  Using cached scikit_base-0.7.8-py3-none-any.whl (130 kB)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from sktime==0.26.0->pycaret) (21.3)\r\nRequirement already satisfied: patsy>=0.5.1 in /databricks/python3/lib/python3.9/site-packages (from category-encoders>=2.4.0->pycaret) (0.5.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from imbalanced-learn>=0.12.0->pycaret) (2.2.0)\r\nCollecting zipp>=3.20\r\n  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (0.18.1)\r\nRequirement already satisfied: backcall in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\r\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (0.1.2)\r\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (2.11.2)\r\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\r\nRequirement already satisfied: traitlets>=5 in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\r\nRequirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (3.0.20)\r\nRequirement already satisfied: stack-data in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\r\nRequirement already satisfied: pickleshare in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\r\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.9/site-packages (from ipython>=5.5.0->pycaret) (4.8.0)\r\nRequirement already satisfied: ipython-genutils~=0.2.0 in /databricks/python3/lib/python3.9/site-packages (from ipywidgets>=7.6.5->pycaret) (0.2.0)\r\nRequirement already satisfied: ipykernel>=4.5.1 in /databricks/python3/lib/python3.9/site-packages (from ipywidgets>=7.6.5->pycaret) (6.15.3)\r\nRequirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /databricks/python3/lib/python3.9/site-packages (from ipywidgets>=7.6.5->pycaret) (1.0.0)\r\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /databricks/python3/lib/python3.9/site-packages (from ipywidgets>=7.6.5->pycaret) (3.6.1)\r\nRequirement already satisfied: jupyter-client>=6.1.12 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1.12)\r\nRequirement already satisfied: tornado>=6.1 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1)\r\nRequirement already satisfied: debugpy>=1.0 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.5.1)\r\nRequirement already satisfied: pyzmq>=17 in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (22.3.0)\r\nRequirement already satisfied: nest-asyncio in /databricks/python3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.5.5)\r\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /databricks/python3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.3)\r\nRequirement already satisfied: jupyter-core>=4.6.0 in /databricks/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (4.11.2)\r\nRequirement already satisfied: python-dateutil>=2.1 in /databricks/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (2.8.2)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<3.8.0->pycaret) (3.0.4)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<3.8.0->pycaret) (0.11.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<3.8.0->pycaret) (1.3.2)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<3.8.0->pycaret) (9.0.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<3.8.0->pycaret) (4.25.0)\r\nRequirement already satisfied: fastjsonschema in /databricks/python3/lib/python3.9/site-packages (from nbformat>=4.2.0->pycaret) (2.16.2)\r\nRequirement already satisfied: jsonschema>=2.6 in /databricks/python3/lib/python3.9/site-packages (from nbformat>=4.2.0->pycaret) (4.4.0)\r\nRequirement already satisfied: attrs>=17.4.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (21.4.0)\r\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.18.0)\r\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/lib/python3.9/site-packages (from numba>=0.55.0->pycaret) (0.43.0)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas<2.2.0->pycaret) (2021.3)\r\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from patsy>=0.5.1->category-encoders>=2.4.0->pycaret) (1.16.0)\r\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=5.5.0->pycaret) (0.7.0)\r\nCollecting narwhals>=1.15.1\r\n  Using cached narwhals-1.30.0-py3-none-any.whl (313 kB)\r\nCollecting plotly>=5.14.0\r\n  Using cached plotly-5.24.1-py3-none-any.whl (19.1 MB)\r\nCollecting tsdownsample>=0.1.3\r\n  Using cached tsdownsample-0.1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\nCollecting orjson<4.0.0,>=3.8.0\r\n  Using cached orjson-3.10.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\r\nCollecting dash>=2.9.0\r\n  Using cached dash-2.18.2-py3-none-any.whl (7.8 MB)\r\nCollecting Werkzeug<3.1\r\n  Using cached werkzeug-3.0.6-py3-none-any.whl (227 kB)\r\nCollecting dash-table==5.0.0\r\n  Using cached dash_table-5.0.0-py3-none-any.whl (3.9 kB)\r\nRequirement already satisfied: typing-extensions>=4.1.1 in /databricks/python3/lib/python3.9/site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (4.1.1)\r\nCollecting dash-core-components==2.0.0\r\n  Using cached dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\r\nCollecting dash-html-components==2.0.0\r\n  Using cached dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\r\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (61.2.0)\r\nCollecting retrying\r\n  Using cached retrying-1.3.4-py3-none-any.whl (11 kB)\r\nCollecting Flask<3.1,>=1.0.4\r\n  Using cached flask-3.0.3-py3-none-any.whl (101 kB)\r\nCollecting blinker>=1.6.2\r\n  Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\r\nCollecting itsdangerous>=2.1.2\r\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\r\nCollecting click>=8.1.3\r\n  Using cached click-8.1.8-py3-none-any.whl (98 kB)\r\nRequirement already satisfied: tenacity>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from plotly>=5.14.0->pycaret) (8.0.1)\r\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.9/site-packages (from pmdarima>=2.0.4->pycaret) (1.26.9)\r\nRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /databricks/python3/lib/python3.9/site-packages (from pmdarima>=2.0.4->pycaret) (0.29.28)\r\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=5.5.0->pycaret) (0.2.5)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.27.1->pycaret) (2.0.4)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.27.1->pycaret) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.27.1->pycaret) (2021.10.8)\r\nCollecting markupsafe>=2.0.1\r\n  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\nRequirement already satisfied: notebook>=4.4.1 in /databricks/python3/lib/python3.9/site-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.4.8)\r\nRequirement already satisfied: nbconvert in /databricks/python3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.4.4)\r\nRequirement already satisfied: argon2-cffi in /databricks/python3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.3.0)\r\nRequirement already satisfied: prometheus-client in /databricks/python3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.13.1)\r\nRequirement already satisfied: terminado>=0.8.3 in /databricks/python3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.13.1)\r\nRequirement already satisfied: Send2Trash>=1.8.0 in /databricks/python3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.0)\r\nRequirement already satisfied: argon2-cffi-bindings in /databricks/python3/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.2.0)\r\nRequirement already satisfied: cffi>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.15.0)\r\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.21)\r\nRequirement already satisfied: entrypoints>=0.2.2 in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.4)\r\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.1.0)\r\nRequirement already satisfied: testpath in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.0)\r\nRequirement already satisfied: pandocfilters>=1.4.1 in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.5.0)\r\nRequirement already satisfied: defusedxml in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.7.1)\r\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.13)\r\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.11.1)\r\nRequirement already satisfied: jupyterlab-pygments in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.1.2)\r\nRequirement already satisfied: mistune<2,>=0.8.1 in /databricks/python3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.8.4)\r\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.3.1)\r\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.1)\r\nRequirement already satisfied: executing in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=5.5.0->pycaret) (0.8.3)\r\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=5.5.0->pycaret) (0.2.2)\r\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.9/site-packages (from stack-data->ipython>=5.5.0->pycaret) (2.0.5)\r\nInstalling collected packages: markupsafe, zipp, psutil, jinja2, Werkzeug, joblib, itsdangerous, importlib-metadata, click, blinker, scikit-learn, retrying, plotly, Flask, dash-table, dash-html-components, dash-core-components, tsdownsample, scikit-base, pmdarima, orjson, dash, yellowbrick, xxhash, wurlitzer, tbats, sktime, scikit-plot, schemdraw, pyod, plotly-resampler, lightgbm, kaleido, imbalanced-learn, deprecation, category-encoders, pycaret\r\n  Attempting uninstall: markupsafe\r\n    Found existing installation: MarkupSafe 2.0.1\r\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\r\n  Attempting uninstall: psutil\r\n    Found existing installation: psutil 5.8.0\r\n    Not uninstalling psutil at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'psutil'. No files were found to uninstall.\r\n  Attempting uninstall: jinja2\r\n    Found existing installation: Jinja2 2.11.3\r\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\r\n  Attempting uninstall: joblib\r\n    Found existing installation: joblib 1.1.1\r\n    Not uninstalling joblib at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'joblib'. No files were found to uninstall.\r\n  Attempting uninstall: click\r\n    Found existing installation: click 8.0.4\r\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'click'. No files were found to uninstall.\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 1.0.2\r\n    Not uninstalling scikit-learn at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\r\n  Attempting uninstall: plotly\r\n    Found existing installation: plotly 5.6.0\r\n    Not uninstalling plotly at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152\r\n    Can't uninstall 'plotly'. No files were found to uninstall.\r\nSuccessfully installed Flask-3.0.3 Werkzeug-3.0.6 blinker-1.9.0 category-encoders-2.6.4 click-8.1.8 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 deprecation-2.1.0 imbalanced-learn-0.12.4 importlib-metadata-8.6.1 itsdangerous-2.2.0 jinja2-3.1.6 joblib-1.3.2 kaleido-0.2.1 lightgbm-4.6.0 markupsafe-3.0.2 orjson-3.10.15 plotly-5.24.1 plotly-resampler-0.10.0 pmdarima-2.0.4 psutil-7.0.0 pycaret-3.3.2 pyod-2.0.3 retrying-1.3.4 schemdraw-0.15 scikit-base-0.7.8 scikit-learn-1.4.2 scikit-plot-0.3.7 sktime-0.26.0 tbats-1.1.3 tsdownsample-0.1.4.1 wurlitzer-3.1.1 xxhash-3.5.0 yellowbrick-1.5 zipp-3.21.0\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-103b205b-fcdb-46b5-bb1c-c9c4b7eb8152/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# pip install synapseml\n",
    "!pip install shap\n",
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e809fa-c387-4877-9827-0dc4c14cc387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np; np.__version__ = '1.24.0'\n",
    "import shap\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import  *\n",
    "\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection  import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3639b1-26a4-47bc-8c82-a84b7c51bbf1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pipeline_Data_ Engineering"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, MinMaxScaler, VectorAssembler\n",
    "\n",
    "\n",
    "categorical_columns = ['policy_state',\n",
    "'policy_csl',\n",
    "'insured_sex',\n",
    "'insured_education_level',\n",
    "'insured_occupation',\n",
    "'insured_hobbies',\n",
    "'insured_relationship',\n",
    "'incident_date',\n",
    "'incident_type',\n",
    "'collision_type',\n",
    "'incident_severity',\n",
    "'authorities_contacted',\n",
    "'incident_state',\n",
    "'incident_city',\n",
    "#'incident_location',\n",
    "'property_damage',\n",
    "'police_report_available',\n",
    "'auto_make',\n",
    "'auto_model',\n",
    "'fraud_reported']\n",
    "\n",
    "\n",
    "numerical_columns= ['injury_claim', 'umbrella_limit', 'insured_zip', 'property_claim', 'witnesses', 'age', 'capital-gains', 'months_as_customer', 'incident_hour_of_the_day', 'policy_number', 'policy_annual_premium', 'auto_year', 'total_claim_amount', 'number_of_vehicles_involved', 'capital-loss', 'bodily_injuries', 'vehicle_claim', 'policy_deductable']\n",
    "\n",
    "# Step 1: StringIndexer and OneHotEncoder for categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_encoded\") for col in categorical_columns]\n",
    "\n",
    "# Step 2: VectorAssembler for numerical columns\n",
    "num_assemblers = [VectorAssembler(inputCols=[col], outputCol=f\"{col}_vec\") for col in numerical_columns]\n",
    "\n",
    "# Step 3: MinMaxScaler for numerical columns (after they are assembled into vectors)\n",
    "scalers = [MinMaxScaler(inputCol=f\"{col}_vec\", outputCol=f\"{col}_normalized\") for col in numerical_columns]\n",
    "\n",
    "# Step 5: Create the full pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + num_assemblers + scalers)\n",
    "\n",
    "\n",
    "\n",
    "# 0311\n",
    "# # Step 1: StringIndexer for categorical columns\n",
    "# sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_index\")\n",
    "# smoker_indexer = StringIndexer(inputCol=\"smoker\", outputCol=\"smoker_index\")\n",
    "# region_indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n",
    "\n",
    "# # Step 2: OneHotEncoder for region\n",
    "# region_encoder = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n",
    "\n",
    "# # Step 3: VectorAssembler to combine \"bmi\" for MinMaxScaler\n",
    "# bmi_assembler = VectorAssembler(inputCols=[\"bmi\"], outputCol=\"bmi_assembled\")\n",
    "# age_assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"age_assembled\")\n",
    "\n",
    "# # Step 4: MinMaxScaler for normalization\n",
    "# bmi_scaler = MinMaxScaler(inputCol=\"bmi_assembled\", outputCol=\"bmi_normalized\")\n",
    "# age_scaler = MinMaxScaler(inputCol=\"age_assembled\", outputCol=\"age_normalized\")\n",
    "\n",
    "# # Step 5: Create Pipeline\n",
    "# pipeline = Pipeline(stages=[\n",
    "#     sex_indexer, smoker_indexer, region_indexer, region_encoder,  # Categorical encoding\n",
    "#     bmi_assembler, bmi_scaler,  # bmi normalization\n",
    "#     age_assembler, age_scaler   # age normalization\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ecea318-cf67-4d29-bf76-29e48d689943",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VectorAssembler"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def vectorize_features(df_transformed):\n",
    "    # Step 1: Select the features to vectorize\n",
    "    input_columns = [\"sex_index\", \"smoker_index\", \"region_encoded\", \"bmi_normalized\", \"age_normalized\", \"children\"]\n",
    "\n",
    "    # Step 2: Create a new VectorAssembler to combine features\n",
    "    assembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "\n",
    "    # Step 3: Rename expenses to label\n",
    "    df_transformed = df_transformed.withColumnRenamed(\"expenses\", \"label\")\n",
    "\n",
    "    # Step 4: Vectorize the features using VectorAssembler\n",
    "    df_vector = assembler.transform(df_transformed)\n",
    "    df_final = df_vector.select(\"label\", \"features\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4573d065-f760-43c0-bf9a-4493ab38783e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "plot_learning_curve"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curve(X_train, y_train,rf, para, start, end, step,cv=3):\n",
    "    results = []\n",
    "    for i in range(start, end, step):\n",
    "        rf.set_params(**{para: i})\n",
    "        scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "        results.append(scores.mean())\n",
    "    plt.plot(range(start, end, step), results, color=\"red\", label=\"auc\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{para}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce8e5b7-db6b-4fb9-bc13-0b966000ca24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shap_Global"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_shap_summary(rf, X_train, X_test, feature_names):\n",
    "    explainer = shap.Explainer(rf, X_train)\n",
    "    shap_values = explainer(X_test, check_additivity=False)\n",
    "    shap.summary_plot(shap_values, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57e9c6d-d5b7-4a30-b570-89617f8611bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shap_sample"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_shap_sample(X_train, X_test,feature_names, sample_idx):\n",
    "    explainer = shap.Explainer(rf, X_train)\n",
    "    shap_values = explainer(X_test, check_additivity=False)\n",
    "    shap_values_for_sample = shap_values[sample_idx]\n",
    "    return shap.force_plot(explainer.expected_value, shap_values_for_sample.values, X_test[sample_idx], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5beee2e4-10a4-41c6-95ce-cbd170193b52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "application_predict_expenses"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def application_predict_expenses(path_saved, age, sex, bmi, children, smoker, region, expenses):\n",
    "    loaded_model = PipelineModel.load(path_saved)\n",
    "    \n",
    "    new_data = spark.createDataFrame([(age, sex, bmi, children, smoker, region, expenses)], [\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\", \"expenses\"])\n",
    "    \n",
    "    predictions = loaded_model.transform(new_data)\n",
    "    \n",
    "    expected_expenses = predictions.withColumn(\"Expected_Expenses\", round(\"prediction\", 2))\n",
    "    expected_expenses = expected_expenses.withColumn(\"Actual_Expenses\", round(\"expenses\", 2))\n",
    "\n",
    "    expected_expenses = expected_expenses.withColumn(\"Claim_Status\", when(col(\"Expected_Expenses\") < col(\"Actual_Expenses\"), \"Abnormal Claim\").otherwise(\"Normal Claim\"))\n",
    "    \n",
    "    display(expected_expenses.select(\"Claim_Status\", \"Expected_Expenses\", \"Actual_Expenses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c99bf6d-ddb9-47ab-8655-b44972b12f87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "For_Local_Notebook"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "\n",
    "# def display(df):\n",
    "#     \"\"\"\n",
    "#     Mimics Databricks' display() function by converting a Spark DataFrame to Pandas \n",
    "#     and displaying it nicely in Jupyter notebooks.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pyspark.sql.DataFrame): The Spark DataFrame to display.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         from IPython.display import display as ipy_display\n",
    "#         ipy_display(df.toPandas())  # Convert to Pandas and display\n",
    "#     except:\n",
    "#         print(df.show())  # Fallback to show() if Pandas conversion fails"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_Function",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
